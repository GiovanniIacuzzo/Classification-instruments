{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# print some statistics\n",
    "print(\"Number of training examples: {}\".format(len(train_dataset)))\n",
    "print(\"Number of test examples: {}\".format(len(test_dataset)))\n",
    "print(\"Number of classes: {}\".format(len(train_dataset.features[\"label\"].names)))\n",
    "print(\"Classes: {}\".format(train_dataset.features[\"label\"].names))\n",
    "\n",
    "# print a sample\n",
    "print(\"Sample: {}\".format(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e209c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_list, label_list, vocab_to_idx, max_length=32, pad_token=\"<PAD>\", unk_token=\"<UNK>\"):\n",
    "        self.text_list = text_list\n",
    "        self.label_list = label_list\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.max_length = max_length\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text_list[index]\n",
    "        label = self.label_list[index]\n",
    "        text = [self.vocab_to_idx[word] if word in self.vocab_to_idx else self.vocab_to_idx[self.unk_token] for word in text.split()]\n",
    "        if len(text) > self.max_length:\n",
    "            text = text[:self.max_length]\n",
    "        else:\n",
    "            text = text + [self.vocab_to_idx[self.pad_token]] * (self.max_length - len(text))\n",
    "        return torch.tensor(text), torch.tensor(label)\n",
    "\n",
    "print(train_dataset[0])\n",
    "train_list = [example[\"text\"] for example in train_dataset]\n",
    "train_label_list = [example[\"label\"] for example in train_dataset]\n",
    "\n",
    "# split the training set into training and validation set\n",
    "train_text_list, val_text_list, train_label_list, val_label_list = train_test_split(train_list, train_label_list, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = Dataset(train_text_list, train_label_list, vocab_to_idx)\n",
    "val_dataset = Dataset(val_text_list, val_label_list, vocab_to_idx)\n",
    "\n",
    "# test dataset\n",
    "test_text_list = [example[\"text\"] for example in test_dataset]\n",
    "test_label_list = [example[\"label\"] for example in test_dataset]\n",
    "test_dataset = Dataset(test_text_list, test_label_list, vocab_to_idx)\n",
    "\n",
    "print(\"Number of training examples: {}\".format(len(train_dataset)))\n",
    "print(\"Number of validation examples: {}\".format(len(val_dataset)))\n",
    "print(\"Number of test examples: {}\".format(len(test_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers)\n",
    "        self.fc = torch.nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7763f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, num_epochs, criterion, optimizer, device, model_name):\n",
    "    best_accuracy = 0\n",
    "    best_accuracy_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        p_bar = tqdm(train_dataloader, desc=\"Epoch {}\".format(epoch + 1))\n",
    "        model.train()\n",
    "        for batch in p_bar:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            p_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        # evaluate on validation set\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_preds.extend(outputs.squeeze(1).tolist())\n",
    "                val_labels.extend(labels.tolist())\n",
    "        accuracy, precision, recall, f1 = compute_metrics(val_preds, val_labels)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_accuracy_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), \"models/{}_model_imdb.pt\".format(model_name))\n",
    "        print(f\"Epoch {epoch + 1} Validation Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return best_accuracy, best_accuracy_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaef733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_layers=2)\n",
    "model.to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "print(\"*\" * 80)\n",
    "print(\"LSTM\")\n",
    "\n",
    "t0 = time.time()\n",
    "best_accuracy, best_accuracy_epoch = train_model(model, train_dataloader, val_dataloader, num_epochs, criterion, optimizer, device, \"bidirectional_lstm_2\")\n",
    "t1 = time.time()\n",
    "print(f\"Best validation accuracy for LSTM: {best_accuracy:.4f} at epoch {best_accuracy_epoch}\")\n",
    "print(f\"Training took {t1 - t0:.4f} seconds\")\n",
    "print(\"*\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
